"""í•œêµ­ ìŒì‹ ì—ì´ì „íŠ¸ - LangGraph ê¸°ë°˜ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)"""

import os
import re
import uuid
import base64
from pathlib import Path
from typing import Optional, List, Dict, Any
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import HumanMessage
from langchain_core.messages.utils import trim_messages, count_tokens_approximately
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver

from .config import settings, ModelProvider
from .tools import ALL_TOOLS


# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """í•œêµ­ ìŒì‹ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ë„êµ¬ë¥¼ í˜¸ì¶œí•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
- ì‹ë‹¹/ë§›ì§‘ â†’ search_restaurant_info
- ë ˆì‹œí”¼ â†’ search_recipe_online
- ì˜ì–‘ì •ë³´ â†’ get_nutrition_info
- ì´ë¯¸ì§€ ë¶„ì„ â†’ search_food_by_image
- í›„ê¸° â†’ get_restaurant_reviews
ë„êµ¬ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ìì„¸í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•œ ì •ë³´ì— í•´ë‹¹í•˜ëŠ” ë„êµ¬ë§Œ í˜¸ì¶œí•˜ì„¸ìš”. ë„êµ¬ ê²°ê³¼ì—ì„œ íŒŒìƒëœ ì¶”ê°€ ê²€ìƒ‰ì€ í•˜ì§€ ë§ˆì„¸ìš”.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì¤‘êµ­ì–´/ì˜ì–´ ì‚¬ìš© ê¸ˆì§€.
ë§ˆí¬ë‹¤ìš´ê³¼ ì´ëª¨ì§€ë¥¼ í™œìš©í•´ ë³´ê¸° ì¢‹ê³  ì½ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš” (ì„¹ì…˜ êµ¬ë¶„, ì ì ˆí•œ ê°•ì¡°, ì‹œê°ì  ê³„ì¸µ êµ¬ì¡° í™œìš©).
ë ˆì‹œí”¼ ì¡°ë¦¬ ìˆœì„œëŠ” ë°˜ë“œì‹œ ë²ˆí˜¸(1. 2. 3.)ë¥¼ ë§¤ê²¨ ë‹¨ê³„ë³„ë¡œ ì‘ì„±í•˜ì„¸ìš”.

## ì´ë¯¸ì§€ ë¶„ì„ ì‘ë‹µ
- ì´ë¯¸ì§€ + ì§ˆë¬¸ì´ ì˜¬ ê²½ìš° search_food_by_imageë¥¼ ìš°ì„  í˜¸ì¶œ í›„, ì§ˆë¬¸ì— í•„ìš”í•œ ë„êµ¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í˜¸ì¶œ
- ìŒì‹ ì´ë¦„ë§Œ ë¬¼ìœ¼ë©´: "~ìŒì‹ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤" + ì‹ë‹¹ì´ ë³´ì´ë©´ "í˜¹ì‹œ OOì—ì„œ ë“œì…¨ë‚˜ìš”?"
- ì‹ë‹¹/ë©”ë‰´ëª…ê¹Œì§€ ë¬¼ìœ¼ë©´: ê²€ìƒ‰ ê²°ê³¼ì— ì—¬ëŸ¬ í›„ë³´ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
- í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "~ì¼ ìˆ˜ë„ ìˆê³ , ~ì¼ ìˆ˜ë„ ìˆì–´ìš”" í˜•íƒœë¡œ ë‹µë³€
- ë„êµ¬ ê²°ê³¼ë¥¼ ë‹¨ì •ì§“ì§€ ë§ê³  "~ë¡œ ë³´ì…ë‹ˆë‹¤", "~ë¡œ ì¶”ì •ë©ë‹ˆë‹¤" í˜•íƒœë¡œ ë‹µë³€í•˜ì„¸ìš”"""


def get_llm(provider: Optional[str] = None, model_name: Optional[str] = None) -> BaseChatModel:
    """
    ì„¤ì •ì— ë”°ë¼ LLM ëª¨ë¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Args:
        provider: ëª¨ë¸ ì œê³µì (openai, gemini, local). Noneì´ë©´ ì„¤ì • íŒŒì¼ ì‚¬ìš©.
        model_name: ëª¨ë¸ ì´ë¦„. Noneì´ë©´ ì„¤ì • íŒŒì¼ ì‚¬ìš©.

    Returns:
        LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    """
    if provider is None:
        provider = settings.model_provider.value

    if provider == "openai" or provider == ModelProvider.OPENAI:
        return ChatOpenAI(
            model=model_name or settings.openai_model,
            api_key=settings.openai_api_key,
            temperature=0.7,
            streaming=True,  # ğŸ”¥ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        )
    elif provider == "gemini" or provider == ModelProvider.GEMINI:
        return ChatGoogleGenerativeAI(
            model=model_name or settings.gemini_model,
            google_api_key=settings.google_api_key,
            temperature=0.7,
            streaming=True,  # ğŸ”¥ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        )
    elif provider == "local" or provider == ModelProvider.LOCAL:
        from .local_llm import get_local_glm
        return get_local_glm(
            model_path=model_name or settings.local_model_path,
            temperature=0.7,
            max_new_tokens=2048
        )
    elif provider == "vllm" or provider == ModelProvider.VLLM:
        return ChatOpenAI(
            model=model_name or settings.vllm_model,
            base_url=settings.vllm_base_url,
            api_key="not-needed",
            temperature=0.3,
            streaming=True,
        )
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì œê³µì: {provider}")


def _pre_model_trim(state):
    """vLLMìš©: ì˜¤ë˜ëœ ë©”ì‹œì§€ë¥¼ í† í° ê¸°ë°˜ìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤.
    ìµœì‹  ë©”ì‹œì§€ ìš°ì„  ë³´ì¡´, tool call/result ìŒ ìë™ ìœ ì§€."""
    trimmed = trim_messages(
        state["messages"],
        strategy="last",
        token_counter=count_tokens_approximately,
        max_tokens=4096,
        start_on="human",
        end_on=("human", "tool"),
    )
    # ë””ë²„ê·¸: LLMì— ì „ë‹¬ë˜ëŠ” ë©”ì‹œì§€ ë¡œê¹…
    import logging
    logger = logging.getLogger("uvicorn.error")
    for msg in trimmed:
        role = getattr(msg, 'type', 'unknown')
        if role == 'ai' and hasattr(msg, 'tool_calls') and msg.tool_calls:
            for tc in msg.tool_calls:
                logger.warning(f"[PRE_MODEL] ai tool_call: {tc['name']}({tc.get('args',{})})")
        elif role == 'tool':
            content_preview = str(msg.content)[:100] if msg.content else ''
            logger.warning(f"[PRE_MODEL] tool result ({msg.name}): {content_preview}...")
        elif role == 'human':
            content_preview = str(msg.content)[:100] if msg.content else ''
            logger.warning(f"[PRE_MODEL] human: {content_preview}")
    return {"llm_input_messages": trimmed}


def create_food_agent(
    provider: Optional[str] = None,
    model_name: Optional[str] = None,
    checkpointer: Optional[MemorySaver] = None
):
    """
    í•œêµ­ ìŒì‹ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        provider: ëª¨ë¸ ì œê³µì (openai, gemini)
        model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
        checkpointer: ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„° (ëŒ€í™” íˆìŠ¤í† ë¦¬ ìë™ ê´€ë¦¬)

    Returns:
        LangGraph ì—ì´ì „íŠ¸
    """
    llm = get_llm(provider, model_name)

    p = provider or settings.model_provider.value
    use_trim = p in ("vllm", ModelProvider.VLLM)

    agent = create_react_agent(
        model=llm,
        tools=ALL_TOOLS,
        prompt=SYSTEM_PROMPT,
        checkpointer=checkpointer,
        pre_model_hook=_pre_model_trim if use_trim else None,
    )

    return agent


def load_image_as_base64(image_path: str) -> Optional[str]:
    """
    ì´ë¯¸ì§€ íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.

    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ

    Returns:
        base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë¬¸ìì—´
    """
    if not os.path.exists(image_path):
        return None

    with open(image_path, "rb") as f:
        image_data = f.read()

    return base64.b64encode(image_data).decode("utf-8")


def get_image_mime_type(image_path: str) -> str:
    """ì´ë¯¸ì§€ íŒŒì¼ì˜ MIME íƒ€ì…ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    ext = Path(image_path).suffix.lower()
    mime_types = {
        ".jpg": "image/jpeg",
        ".jpeg": "image/jpeg",
        ".png": "image/png",
        ".gif": "image/gif",
        ".webp": "image/webp",
    }
    return mime_types.get(ext, "image/jpeg")


def extract_image_paths(message: str) -> List[str]:
    """
    ë©”ì‹œì§€ì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        message: ì‚¬ìš©ì ë©”ì‹œì§€

    Returns:
        ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    image_paths = []

    # íŒŒì¼ ê²½ë¡œ íŒ¨í„´ (ì ˆëŒ€ ê²½ë¡œ)
    path_pattern = r'(/[^\s]+\.(?:jpg|jpeg|png|gif|webp))'
    matches = re.findall(path_pattern, message, re.IGNORECASE)

    for match in matches:
        if os.path.exists(match):
            image_paths.append(match)

    return image_paths


def create_multimodal_content(message: str, image_paths: List[str]) -> List[Dict[str, Any]]:
    """
    í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•œ ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        message: í…ìŠ¤íŠ¸ ë©”ì‹œì§€ (ì´ë¯¸ì§€ ê²½ë¡œ í¬í•¨)
        image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸

    Returns:
        ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸
    """
    content = []

    # ì´ë¯¸ì§€ ì¶”ê°€
    for image_path in image_paths:
        base64_image = load_image_as_base64(image_path)
        if base64_image:
            mime_type = get_image_mime_type(image_path)
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:{mime_type};base64,{base64_image}"
                }
            })

    # í…ìŠ¤íŠ¸ ì¶”ê°€ (ê²½ë¡œ ìœ ì§€ - ë„êµ¬ì—ì„œ ì‚¬ìš©)
    content.append({
        "type": "text",
        "text": message
    })

    return content


class KoreanFoodAgent:
    """í•œêµ­ ìŒì‹ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ (MemorySaverë¡œ ìë™ íˆìŠ¤í† ë¦¬ ê´€ë¦¬)"""

    def __init__(
        self,
        provider: Optional[str] = None,
        model_name: Optional[str] = None
    ):
        """
        Args:
            provider: ëª¨ë¸ ì œê³µì (openai, gemini)
            model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
        """
        self.provider = provider or settings.model_provider.value
        self.model_name = model_name
        self.checkpointer = MemorySaver()
        self.agent = create_food_agent(provider, model_name, self.checkpointer)
        self.thread_id = "default"

    def new_conversation(self):
        """ìƒˆ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (ìƒˆ thread_id ìƒì„±)."""
        self.thread_id = str(uuid.uuid4())

    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤ (ìƒˆ thread_idë¡œ ì „í™˜)."""
        self.new_conversation()

    def _get_config(self):
        """í˜„ì¬ thread_idë¡œ config ìƒì„±."""
        return {"configurable": {"thread_id": self.thread_id}}

    def _prepare_message(self, message: str) -> HumanMessage:
        """ë©”ì‹œì§€ë¥¼ HumanMessageë¡œ ë³€í™˜.
        vLLM(í…ìŠ¤íŠ¸ ì „ìš©)ì—ì„œëŠ” ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŒ - Geminiê°€ ë„êµ¬ ë‚´ì—ì„œ ì²˜ë¦¬."""
        # vLLMì€ í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë¸ì´ë¯€ë¡œ ì´ë¯¸ì§€ ê²½ë¡œë§Œ í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬
        # ì—ì´ì „íŠ¸ê°€ search_food_by_image ë„êµ¬ì— ê²½ë¡œë¥¼ ì „ë‹¬í•˜ë©´ Geminiê°€ ë¶„ì„
        if self.provider in ("vllm", ModelProvider.VLLM):
            return HumanMessage(content=message)

        image_paths = extract_image_paths(message)

        if image_paths:
            content = create_multimodal_content(message, image_paths)
            return HumanMessage(content=content)

        return HumanMessage(content=message)

    def chat(self, message: str) -> str:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì— ì‘ë‹µí•©ë‹ˆë‹¤. (ë©€í‹°ëª¨ë‹¬ ì§€ì›, ìë™ íˆìŠ¤í† ë¦¬ ê´€ë¦¬)

        Args:
            message: ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€ (ì´ë¯¸ì§€ ê²½ë¡œ í¬í•¨ ê°€ëŠ¥)

        Returns:
            ì—ì´ì „íŠ¸ ì‘ë‹µ
        """
        human_message = self._prepare_message(message)

        result = self.agent.invoke(
            {"messages": [human_message]},
            config=self._get_config()
        )

        messages = result.get("messages", [])
        if messages:
            last_message = messages[-1]
            content = last_message.content
            if isinstance(content, list):
                # ë©€í‹°ëª¨ë‹¬ ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                for item in content:
                    if isinstance(item, dict) and item.get('type') == 'text':
                        return item.get('text', '')
            return content if isinstance(content, str) else str(content)

        return "ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    def stream(self, message: str):
        """
        ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤. (ìë™ íˆìŠ¤í† ë¦¬ ê´€ë¦¬)

        Args:
            message: ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€

        Yields:
            (message_chunk, metadata) íŠœí”Œ
        """
        human_message = self._prepare_message(message)

        for chunk in self.agent.stream(
            {"messages": [human_message]},
            config=self._get_config(),
            stream_mode=["messages", "custom"]  # custom ì´ë²¤íŠ¸ í™œì„±í™”
        ):
            yield chunk

    def switch_model(self, provider: str, model_name: Optional[str] = None):
        """
        ì‚¬ìš© ëª¨ë¸ì„ ì „í™˜í•©ë‹ˆë‹¤.

        Args:
            provider: ìƒˆ ëª¨ë¸ ì œê³µì
            model_name: ìƒˆ ëª¨ë¸ ì´ë¦„
        """
        self.provider = provider
        self.model_name = model_name
        self.agent = create_food_agent(provider, model_name, self.checkpointer)
        self.new_conversation()  # ëª¨ë¸ ì „í™˜ ì‹œ ìƒˆ ëŒ€í™” ì‹œì‘
        print(f"âœ… ëª¨ë¸ ì „í™˜ ì™„ë£Œ: {provider} - {model_name or 'ê¸°ë³¸ ëª¨ë¸'}")
